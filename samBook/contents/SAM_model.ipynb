{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Komponenten\n",
    "\n",
    "```{figure} ./images/model_aufbau.png\n",
    ":name: model_aufbau\n",
    ":align: center\n",
    "Komponenten des Segment Anything Models. Image Encoder, Prompt Encoder, and Mask Decoder. {cite}`kirillov2023segment`\n",
    "```\n",
    "\n",
    "### 1. Image Encoder\n",
    "- Erstellt **Image Embeddings** für Input-Bilder\n",
    "- In Theorie kann jede Art von Image Encoder verwendet werden\n",
    "    - Vorausgesetzt Output ist ein (C×H×W) Embedding\n",
    "- **Vision Transformer** (ViT) von {cite}`dosovitskiy2021image`. Angepasst für hochauflösende Input Bilder\n",
    "    - pre-trained mit **Masked Autoencoder** (MAE) Verfahren\n",
    "    - ViT-H/16\n",
    "    - 14x14 windowed attention Blocks\n",
    "    - 4 global attention Blocks\n",
    "\n",
    "```{figure} ./images/ImageEncoderDiagram.png\n",
    ":name: ImageEncoderDiagram\n",
    ":align: center\n",
    "SAM Image Decoder Struktur im Detail. {cite}`kirillov2023segment`. Eigene Darstellung\n",
    "```\n",
    "\n",
    "### 2. Prompt Encoder\n",
    "- Prompts werden je nach Art unterschiedlich encoded\n",
    "\n",
    "| Type   | Prompt | Embedding                                  |\n",
    "|--------|--------|--------------------------------------------|\n",
    "| Sparse | Points | Positional Encoding + gelernte Embeddings |\n",
    "| Sparse | Boxes  | Positional Encoding + gelernte Embeddings |\n",
    "| Sparse | Text   | CLIP Encoder                               |\n",
    "| Dense  | Mask   | Convolution Embedding + Image Embedding    |\n",
    "\n",
    "- Sparse Prompts werden auf ein **256-dimensional vectorial Embedding** gemappt:\n",
    "    - **Points:** Positional Encodings der Koordinaten summiert mit trainierten Embeddings für Vorder- bzw. Hintergrund.\n",
    "    - **Box:** Embedding Paar: Positional Encoding von Koordinaten \"Oben Links\" und \"Unten Rechts\" werden mit zugehörigen gelernten Embeddings summiert.\n",
    "    - **Text:** CLIP Encoder. Jedoch jeder Text Encoder theoretisch möglich.\n",
    "- **Masks** (Dense Prompts) werden gedownscaled und durch mehrere Convolution Layers transformiert (Siehe Abbildung).\n",
    " \n",
    "```{figure} ./images/MaskPromptEncoding.png\n",
    ":name: MaskPromptEncoder\n",
    ":align: center\n",
    "SAM Mask Prompt Encoding Struktur. {cite}`kirillov2023segment`. Eigene Darstellung\n",
    "```\n",
    "\n",
    "\n",
    "### 3. Mask Decoder\n",
    "- Mappt Image Encoding, Prompt Encoding und ein Output Token auf eine **Maske**.\n",
    "    - Vor Decoding wird dem Prompt Embedding ein trainiertes **Output Token Embedding** hinzugefügt.\n",
    "    - Das **Output Token Embedding** ist eine lernbare Repräsentation der Output Maske.\n",
    "- Modifizierter **Transformer Decoder Block** gefolgt von einem **Dynamic Mask Prediction Head**.\n",
    " \n",
    "- Inspiriert von Transformer Architekturen von {cite}`carion2020endtoend` und {cite}`cheng2021perpixel`.\n",
    "\n",
    "```{figure} ./images/mask_decoder_model.png\n",
    ":name: mask_decoder_model\n",
    ":align: center\n",
    "SAM Lightweight Mask Decoder. {cite}`kirillov2023segment`\n",
    "```\n",
    "\n",
    "#### Decoder Aufbau\n",
    "- **2 Decoder Layer** kombinieren Informationen aus Image- und Prompt Encodings.\n",
    "    - Self-Attention wird auf Tokens (Prompt Encoding + Output Token) angewendet.\n",
    "    - Cross-Attention zwischen Image Encoding und Tokens in beide Richtungen.\n",
    "- Anschließendes 4x upscaling der Image Embeddings und weitere Cross-Attention ausgehend von den Token Embeddings.\n",
    "- **Dynamic Mask Prediction Head** gibt Maske als Output aus.\n",
    "- **Zweiter Head** wird trainiert den **IoU** (Intersection Over Union) des Outputs und der Ground-Truth Maske vorherzusagen.\n",
    "\n",
    "\n",
    "\n",
    "`````{admonition} Cross-Attention in SAM\n",
    ":class: tip\n",
    "Im Mask Decoder von SAM wird **Cross-Attention in beide Richtungen** angewendet (Image to Prompt & Prompt to Image).\n",
    "Am Beispiel Prompt to Image werden **Key** und **Value** anhand von Image Embeddings trainiert während **Query** von Prompt Embeddings selbst gelernt wird. \n",
    "`````\n",
    "\n",
    "\n",
    "`````{admonition} Vorhersage von mehreren Masken\n",
    ":class: tip\n",
    "Um auf die **Mehrdeutigkeit von Prompts** eingehen zu können, können mit SAM mehrere Masken gleichzeitig für einen Prompt berechnet werden. Dazu werden, anstatt einem, **mehrere Output Tokens** an die Prompt Encodings zu Beginn des Decoders angehängt. Pro Output Token wird demnach eine Output Maske berechnet.\n",
    "`````\n",
    "\n",
    "#### Erkenntnisse während des Trainings\n",
    "- Um ausreichend **geometrische Informationen** im Encoder zu erhalten, müssen Image Embeddings immer wenn diese in einem Attention Layer vorkommen mit **Positional Encodings** versehen werden.\n",
    "- Die **originalen Prompt Tokens** (vor Eingabe im Decoder) werden bei jeder Aktualisierung der Tokens (Prompt Encoding + Output Token) im Decoder zusätzlich hinzugefügt um besser **Informationen über den Typ und die Position** des Prompts zu erhalten: *\"This allows for a strong dependence on both the prompt token’s geometric location and type\"*.{cite}`kirillov2023segment`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Nutzung eines **Interactive Segmentation Setup** angelehnt an {cite}`sofiiuk2021reviving` und {cite}`forte2020getting`.\n",
    "- **11 Interationen** pro Trainingsschritt:\n",
    "    - Erste Prediction mit Bounding Box oder Point Prompt.\n",
    "    - 8 Iterative Predictions mit **vorheriger Output Maske** und **gesampelten Punkten** der Prediction Error Region.\n",
    "    - Zwei zusätzliche Iterationen ohne zusätzliche Punkte (Nur vorherige Output Maske).\n",
    "    \n",
    "`````{admonition} Lightweight Mask Decoder\n",
    ":class: note\n",
    "Durch die gute Performance des Decoders konnten in SAM deutlich **mehr Iterationen pro Trainingsschritt** als in vorherigen Interactive Segmentation Projekten verwendet werden.\n",
    "`````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsablauf\n",
    "#### Erste Iteration\n",
    "```{figure} ./images/step1.png\n",
    ":name: training_iterations_1\n",
    ":align: center\n",
    "Erste Maskenvorhersage eines Trainingsschritts. Eigene Darstellung\n",
    "```\n",
    "\n",
    "#### Vergleich mit der Ground Truth\n",
    "```{figure} ./images/step2.png\n",
    ":name: training_iterations_2\n",
    ":align: center\n",
    "Vergleich der ersten Maske mit der Ground Truth Maske. Eigene Darstellung\n",
    "```\n",
    "\n",
    "#### Zweite Iteration\n",
    "```{figure} ./images/step3.png\n",
    ":name: training_iterations_3\n",
    ":align: center\n",
    "Zweite Maskenvorhersage mit zusätzlichen Prompts aus dem ersten Output. Eigene Darstellung\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Um das Model auf jegliche Arten von Anwendungen zu traineren, wurden initiale Box Prompts mit **Random-Noise** versehen, um sowohl mit Boxen zu trainieren, die zu klein als auch zu groß für das Zielobjekt sind.\n",
    "- Um mehr Informationen im Training zu erhalten, wurden **Masken Input Prompts als Wahrscheinlichkeiten** und nicht als binäre Werte in den Decoder übergeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsparameter\n",
    "- AdamW Optimizer\n",
    "- Initiale Lernrate von 0.0008\n",
    "- Batchsize von 256 Bildern\n",
    "- Layer-wise Lernrate Decay von 0.8\n",
    "- Weight Decay von 0.1\n",
    "\n",
    "`````{admonition} Loss\n",
    ":class: note\n",
    "SAM benutzt eine Kombination von **Focal Loss** und **Dice Loss**, um seine Gewichte zu trainieren.\n",
    "`````\n",
    "\n",
    "&rarr; SAM wird in der finalen Version für **90.000 Iterationen** trainiert (ca. 2 SA-1B Epochen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Komponenten\n",
    "\n",
    "```{figure} ./images/model_aufbau.png\n",
    ":name: model_aufbau\n",
    ":align: center\n",
    "Komponenten des Segment Anything Models. Image Encoder, Prompt Encoder, and Mask Decoder. {cite}`kirillov2023segment`\n",
    "```\n",
    "\n",
    "### 1. Image Encoder\n",
    "- Erstellt **Image Embeddings** für Input-Bilder\n",
    "- In Theorie kann jede Art von Image Encoder verwendet werden\n",
    "    - Vorausgesetzt Output ist ein (C×H×W) Embedding\n",
    "- **Vision Transformer** (ViT) von {cite}`dosovitskiy2021image`. Angepasst für hochauflösende Input Bilder\n",
    "    - pre-trained mit **Masked Autoencoder** (MAE) Verfahren\n",
    "    - ViT-H/16\n",
    "    - 14x14 windowed attention Blocks\n",
    "    - 4 global attention Blocks\n",
    "\n",
    "```{figure} ./images/ImageEncoderDiagram.png\n",
    ":name: ImageEncoderDiagram\n",
    ":align: center\n",
    "SAM Image Decoder Struktur im Detail. {cite}`kirillov2023segment`. Eigene Darstellung\n",
    "```\n",
    "\n",
    "### 2. Prompt Encoder\n",
    "- Prompts werden je nach Art unterschiedlich encoded\n",
    "\n",
    "| Type   | Prompt | Embedding                                  |\n",
    "|--------|--------|--------------------------------------------|\n",
    "| Sparse | Points | Positional Encoding + gelernte Embeddings |\n",
    "| Sparse | Boxes  | Positional Encoding + gelernte Embeddings |\n",
    "| Sparse | Text   | CLIP Encoder                               |\n",
    "| Dense  | Mask   | Convolution Embedding + Image Embedding    |\n",
    "\n",
    "- Sparse Prompts werden auf ein **256-dimensional vectorial Embedding** gemappt:\n",
    "    - **Points:** Positional Encodings der Koordinaten summiert mit trainierten Embeddings für Vorder- bzw. Hintergrund.\n",
    "    - **Box:** Embedding Paar: Positional Encoding von Koordinaten \"Oben Links\" und \"Unten Rechts\" werden mit zugehörigen gelernten Embeddings summiert.\n",
    "    - **Text:** CLIP Encoder. Jedoch jeder Text Encoder theoretisch möglich.\n",
    "- **Masks** (Dense Prompts) werden gedownscaled und durch mehrere Convolution Layers transformiert (Siehe Abbildung).\n",
    " \n",
    "```{figure} ./images/MaskPromptEncoding.png\n",
    ":name: MaskPromptEncoder\n",
    ":align: center\n",
    "SAM Mask Prompt Encoding Struktur. {cite}`kirillov2023segment`. Eigene Darstellung\n",
    "```\n",
    "\n",
    "\n",
    "### 3. Mask Decoder\n",
    "- Mappt Image Encoding, Prompt Encoding und ein Output Token auf eine Mask\n",
    "    - Vor Decoding wird dem Prompt Embedding ein trainiertes **Output Token Embedding** hinzugefügt.\n",
    "- Modifizierter **Transformer Decoder Block** gefolgt von einem **Dynamic Mask Prediction Head**\n",
    " \n",
    "- Inspiriert von Transformer Architekturen von {cite}`carion2020endtoend` und {cite}`cheng2021perpixel`\n",
    "\n",
    "```{figure} ./images/mask_decoder_model.png\n",
    ":name: mask_decoder_model\n",
    ":align: center\n",
    "SAM Lightweight Mask Decoder. {cite}`kirillov2023segment`\n",
    "```\n",
    "\n",
    "\n",
    "`````{admonition} Cross-Attention in SAM\n",
    ":class: tip\n",
    "Im Mask Decoder von SAM wird **Cross-Attention in beide Richtungen** angewendet (Image to Prompt & Prompt to Image).\n",
    "Am Beispiel Prompt to Image werden **Key** und **Value** anhand von Image Embeddings trainiert wärend **Query** von Prompt Embeddings selbst gelernt wird. \n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Nutzung eines **Interactive Segmentation Setup** angelehnt an {cite}`sofiiuk2021reviving` und {cite}`forte2020getting`.\n",
    "- **11 Interationen** pro Trainingsschritt:\n",
    "    - Erste Prediction mit Bounding Box oder Point Prompt\n",
    "    - 8 Iterative Predictions mit **vorheriger Output Maske** und **gesampelten Punkten** der Prediction Error Region\n",
    "    - Zwei zusätzliche Iterationen ohne zusätzliche Punkte (Nur vorherige Output Maske)\n",
    "    \n",
    "- Durch die gute Performance des Decoders können deutlich mehr Iterationen pro Trainingsschritt als in vorherigen Interactive Segmentation Projekten verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsablauf\n",
    "#### Erste Iteration\n",
    "```{figure} ./images/step1.png\n",
    ":name: training_iterations_1\n",
    ":align: center\n",
    "Erste Maskenvorhersage eines Trainingsschritts. Eigene Darstellung\n",
    "```\n",
    "\n",
    "#### Vergleich mit der Ground Truth\n",
    "```{figure} ./images/step2.png\n",
    ":name: training_iterations_2\n",
    ":align: center\n",
    "Vergleich der ersten Maske mit der Ground Truth Maske. Eigene Darstellung`\n",
    "```\n",
    "\n",
    "#### Zweite Iteration\n",
    "```{figure} ./images/step3.png\n",
    ":name: training_iterations_3\n",
    ":align: center\n",
    "Zweite Maskenvorhersage mit zusätzlichen Prompts aus dem ersten Output. Eigene Darstellung`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsparameter\n",
    "- AdamW Optimizer\n",
    "- Initiale Lernrate von 0.0008\n",
    "- Batchsize von 256 Bildern\n",
    "- Layer-wise Lernrate Decay von 0.8\n",
    "- Weight Decay von 0.1\n",
    "\n",
    "`````{admonition} Loss\n",
    ":class: note\n",
    "SAM benutzt eine Kombination von **Focal Loss** und **Dice Loss**, um seine Gewichte zu trainieren\n",
    "`````\n",
    "\n",
    "&rarr; SAM wird in der finalen Version für **90.000 Iterationen** trainiert (ca. 2 SA-1B Epochen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

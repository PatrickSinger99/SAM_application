{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engine und Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engine\n",
    "\n",
    "`````{admonition} Data Engine\n",
    ":class: note\n",
    "System, das qualitative, diverse und große **Datasets** für KI Modelle **erstellt**\n",
    "`````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engine für SAM\n",
    "\n",
    "`````{admonition} Problem\n",
    ":class: warning\n",
    "**Datenbeschaffung** &rarr; Wenig Bilder mit annotierten Masken online verfügbar.\n",
    "`````\n",
    "\n",
    "`````{admonition} Lösung\n",
    ":class: tip\n",
    "**Datensätze erstellen** und **Modell co-entwickeln** <br>\n",
    "    &rarr; Datensätze werden während der Entwicklung des Modells annotiert\n",
    "    &rarr; \"Model-in-the-loop\"\n",
    "`````\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau \n",
    "\n",
    "**3 Phasen:**\n",
    "1. Assists Manual\n",
    "2. Semi Automatic\n",
    "3. Fully Automatic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Assists Manual\n",
    "&rarr; **menschliche Annotators labeln manuell Masken**\n",
    "\n",
    "- Browser-based **interactive segmentation tool**\n",
    "    - real-time app powered by SAM\n",
    "    - Menschen **kennzeichnen Masken**, indem sie auf **Punkte von Objekten** im Vorder-/Hintergrund klicken\n",
    "- benennbare oder beschreibbare Objekte\n",
    "- keine semantischen Einschränkungen\n",
    "- kein Speichern der Labels/Beschreibungen, nur der Punkte und Masken\n",
    "\n",
    "`````{admonition} Interactive Segmentation\n",
    ":class: note\n",
    "... nimmt **Bilder** und **Nutzeranmerkungen** als Input, um Masken von Nutzerinteresse zu erzeugen &rarr; **\"Human-in-the-Loop-Methode\"** <br>\n",
    "**Supervised Learning** &rarr; Nutzer verfeinert Segmentierungsergebnisse mit zusätzlichen Informationen {cite}`hao2022eiseg`\n",
    "`````\n",
    "\n",
    "##### Training von SAM\n",
    "1. mit öffentlichen Segmentation Datasets\n",
    "2. neu annotierte Masken \n",
    "    - 6 mal trainiert mit neuen Masken\n",
    "    - Upgrade bei Image Encoder von ViT-B auf ViT-H <br>\n",
    "&rarr; **Inferenz Zeit** pro Maske: von 34 Sekunden auf **14 Sekunden** <br>\n",
    "&rarr; **Masken pro Bild** von 20 auf **44 Masken**\n",
    "\n",
    "**Insgesamt in dieser Phase &rarr; 4,3 Milionen Masken von 120 K Bildern**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Semi Automatic\n",
    "\n",
    "&rarr; **Diversität der Masken für bessere Segmentierung erhöhen**\n",
    "\n",
    "- automatische Erkennung von verlässlichen Masken mit SAM\n",
    "    - Bounding-Box Detector basierend auf Masken der Phase 1 mit \"Object\" Kategorie\n",
    "- menschliche Annotator ergänzen weitere Masken\n",
    "\n",
    "##### Training von SAM\n",
    "- 5 mal trainiert mit neuen Masken\n",
    "&rarr; **Inferenz Zeit** pro Maske: wieder auf **34 Sekunden**, wegen höherer Komplexität <br>\n",
    "&rarr; **Masken pro Bild** von 44 auf **72 Masken**\n",
    "\n",
    "**Insgesamt in dieser Phase &rarr; 5,9 Milionen Masken von 180 K Bildern**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Fully Automatic\n",
    "\n",
    "**&rarr; voll-automatische Annotierung der Masken**\n",
    "\n",
    "- **genug Masken** der vorherigen Phasen\n",
    "- **ambuiguity-aware** Modell entwickelt\n",
    "\n",
    "##### Mehrdeutigkeits-Modell\n",
    "- 32 x 32 Grid an Punkten\n",
    "- für jeden Punkt ein Set an Masken für valide Objekte generieren\n",
    "- wenn Punkt auf Teil oder Teilbereich eines Objekts\n",
    "    &rarr; Model gibt Teilbereich, den Teil und gesamtes Objekt zurück\n",
    "- nur *confident* Masken\n",
    "- Duplikate rausfiltern"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Insgesamt &rarr; 1,1 Miliarde Masken von 11 Milionen K Bildern**\n",
    "\n",
    "### Qualität der Masken\n",
    "- Masken wurden mit 500 **randomly sampled Bildern** (~50k Masken) getestet\n",
    "- menschliche Annotators sollten automatischen Masken verbessern\n",
    "- Vergleich von Paar mit automatisch erkannten und professionell annotierten Masken\n",
    "    &rarr; IoU für jedes Paar &rarr; **94% der Paare haben höher als 90% IoU**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./images/grid_video.gif\n",
    ":name: grid_video\n",
    "Showcase of point grid for automatically segmenting everything in an image. {cite}`AI_2023`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset SA-1B\n",
    "\n",
    "- Verfügbar unter https://ai.facebook.com/datasets/segment-anything/\n",
    "- 1,1 Milliarde Masken von 11 Millionen Bildern\n",
    "- wurde komplett automatisch mit der Data Engine gesammelt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilder\n",
    "\n",
    "- **11 Milionen Bilder** von gewerblichem Anbieter\n",
    "- High Resolution (3300x4950) Bilder &rarr; \n",
    "    - Problem von Accessibility und Storage\n",
    "    - Bilder deswegen **downsampled**\n",
    "- auch nach downsampling, haben die Bilder in SA-1B **viel höhere Auflösung als in anderen bekannten Datensätzen**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masken\n",
    "\n",
    "- **1,1 Miliarden Masken**\n",
    "- **99,1% von SAM generiert**\n",
    "- verglichen mit professionellen Annotations und anderen Datasets  \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./images/dataset_example_1.png\n",
    ":name: dataset_example_1\n",
    ":align: center\n",
    "Beispielbilder des SA-1B Datasets für ein Bild mit bis zu 50 Masken {cite}`kirillov2023segment`.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./images/dataset_example_2.png\n",
    ":name: dataset_example_2\n",
    ":align: center\n",
    "Beispielbilder des SA-1B Datasets für ein Bild mit über 500 Masken {cite}`kirillov2023segment`.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsible AI Analysis\n",
    "\n",
    "**&rarr; Untersuchung möglicher Fairnessprobleme und Verzerrungen bei der Verwendung von SA-1B und SAM**\n",
    "\n",
    "### Geografische Repräsentation\n",
    "```{figure} ./images/responsible_ai_1.png\n",
    ":name: responsible_ai_1\n",
    ":align: center\n",
    "Geschätzte geografische Verteilung der SA-1B-Bilder. {cite}`kirillov2023segment`.\n",
    "```\n",
    "\n",
    "### Fairness bei der Segmentierung von Menschen\n",
    "```{figure} ./images/responsible_ai_2.png\n",
    ":name: responsible_ai_2\n",
    ":width: 400px\n",
    "height: 300px\n",
    ":align: center\n",
    "SAMs Performance bei der Segmentierung von Menschen anhand wahrgenommener Geschlechtszugehörigkeit, Altersgruppe und Hautfarbe. {cite}`kirillov2023segment`.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
